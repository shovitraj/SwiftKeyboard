q.corpus <- quanteda::corpus(data.01)
# get N-grams using quanteda::dfm
dfm.1gram <- dfm(q.corpus, removePunct = TRUE, concatenator = " ")
dfm.2gram <- dfm(q.corpus, removePunct = TRUE,  concatenator = " ", ngrams = 2)
dfm.3gram <- dfm(q.corpus, removePunct = TRUE,  concatenator = " ", ngrams = 3)
# plot a word-cloud of the unigram (1-gram)
textplot_wordcloud(dfm.1gram, max.words = 100, random.order = FALSE,
rot.per =.2, scale = c(8, 1),
colors = RColorBrewer::brewer.pal(8,"Dark2"))
# get most common N-grams (ranked values)
top.1gram <- topfeatures(dfm.1gram, 20)
top.2gram <- topfeatures(dfm.2gram, 20)
top.3gram <- topfeatures(dfm.3gram, 20)
# top.1gram
# top.2gram
# top.3gram
# plot
barplot(sort(top.1gram), xlab = "count", main = "top-20 1-gram", cex.names = 0.7, horiz=TRUE, las =1)
barplot(sort(top.2gram), xlab = "count", main = "top-20 2-gram", cex.names = 0.7, horiz=TRUE, las =1)
barplot(sort(top.3gram), xlab = "count", main = "top-20 3-gram", cex.names = 0.7,
horiz=TRUE, las =1, mgp = c(3, 0, 1))
top.1gram
top.1gram<- top.1gram %>%
filter(n > 20)
# load required packages
require(tm)
require(stringi)
require(data.table)
require(quanteda)
require(ggplot2)
require(wordcloud)
require(tidyverse)
top.1gram<- top.1gram %>%
filter(n > 20)
top.1gram<- top.1gram %>%
top_n(20)
top.1gram
sort(top.1gram)
table(top.1gram)
sort(top.1gram)
onegram <- top.1gram %>%
count(sort=TRUE)
onegram <- top.1gram %>%
count(top.1gram, sort=TRUE)
sort(top.1gram)
sort(top.1gram, decreasing = TRUE)
sort(top.1gram, decreasing = TRUE)
ggplot(data=top.1gram, aes(x=count))
library(data.table)
library(quanteda)
library(dplyr)
uni_words<- readRDS("~/Desktop/Data/final/en_US/Clean/uni_words.rds")
bi_words <- readRDS( "~/Desktop/Data/final/en_US/Clean/bi_words.rds")
tri_words <- readRDS("~/Desktop/Data/final/en_US/Clean/tri_words.rds")
quad_words <- readRDS("~/Desktop/Data/final/en_US/Clean/quad_words.rds")
setkey(uni_words, word_1)
setkey(bi_words, word_1, word_2)
setkey(bi_words, word_1, word_2)
setkey(tri_words, word_1, word_2, word_3)
setkey(quad_words, word_1, word_2, word_3, word_4)
discount_value <- 0.75
# Finding number of bi-gram words
numOfBiGrams <- nrow(bi_words[by = .(word_1, word_2)])
# Dividing number of times word 2 occurs as second part of bigram, by total number of bigrams.
# ( Finding probability for a word given the number of times it was second word of a bigram)
ckn <- bi_words[, .(Prob = ((.N) / numOfBiGrams)), by = word_2]
setkey(ckn, word_2)
# Assigning the probabilities as second word of bigram, to unigrams
uni_words[, Prob := ckn[word_1, Prob]]
uni_words <- uni_words[!is.na(uni_words$Prob)]
# Finding number of times word 1 occurred as word 1 of bi-grams
n1wi <- bi_words[, .(N = .N), by = word_1]
setkey(n1wi, word_1)
# Assigning total times word 1 occured to bigram cn1
bi_words[, Cn1 := uni_words[word_1, count]]
# Kneser Kney Algorithm
bi_words[, Prob := ((count - discount_value) / Cn1 + discount_value / Cn1 * n1wi[word_1, N] * uni_words[word_2, Prob])]
# Finding count of word1-word2 combination in bigram
tri_words[, Cn2 := bi_words[.(word_1, word_2), count]]
# Finding count of word1-word2 combination in trigram
n1w12 <- tri_words[, .N, by = .(word_1, word_2)]
setkey(n1w12, word_1, word_2)
# Kneser Kney Algorithm
tri_words[, Prob := (count - discount_value) / Cn2 + discount_value / Cn2 * n1w12[.(word_1, word_2), N] *
bi_words[.(word_1, word_2), Prob]]
# Finding the most frequently used 50 unigrmas
uni_words <- uni_words[order(-Prob)][1:50]
head(uni_words)
bi_words <- bi_words[order(-Prob)][1:50]
head(bi_words)
# Finding number of bi-gram words
numOfBiGrams <- nrow(bi_words[by = .(word_1, word_2)])
head(numOfBiGrams)
numOfBiGrams
library(dplyr)
library(data.table)
library(ngram)
library(stringi)
library(R.utils)
library(quanteda)
library(readtext)
library(cowplot)
library(reshape2)
library(doParallel)
startCluster<- function(){
cores = detectCores() -1
cluster<- makeCluster(cores)
registerDoParallel(cluster)
return (cluster)
}
cluster <- startCluster()
rm(list=ls())
if(!file.exists("~/Desktop/Data")){
dir.create("~/Desktop/Data")
}
url <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
filedest= "~/Desktop/Data/Coursera-SwiftKey.zip"
if(!file.exists("~/Desktop/Data/Coursera-SwiftKey.zip")){
download.file(url,filedest, mode = "wb")
}
if(!file.exists("~/Desktop/Data/final")){
unzip(zipfile="~/Desktop/Data/Coursera-SwiftKey.zip",exdir="~/Desktop/Data")
}
#file Path
blogs_path <- "~/Desktop/Data/final/en_US/en_US.blogs.txt"
twitter_path <- "~/Desktop/Data/final/en_US/en_US.twitter.txt"
news_path <- "~/Desktop/Data/final/en_US/en_US.news.txt"
#read lines
blogs<-readLines(blogs_path,warn=FALSE,encoding="UTF-8")
twitter<-readLines(twitter_path,warn=FALSE,encoding="UTF-8")
news<-readLines(news_path,warn=FALSE,encoding="UTF-8")
set.seed(165)
#selecting ten percent of data as training data
samplesize <- 0.2
#Sample for each source
#Sample for each source
blogsS <- blogs[sample(1:length(blogs), length(blogs) * samplesize)]
twitterS<- twitter[sample(1:length(twitter), length(twitter) * samplesize)]
newsS <- news[sample(1:length(news), length(news) * samplesize)]
writeLines(c(blogsS, twitterS, newsS), 'sample.txt')
if(!file.exists("./swearWords.txt")){
download.file(url = "https://www.cs.cmu.edu/~biglou/resources/bad-words.txt",
destfile= "./swearWords.txt",
method = "curl")
}
profanity <- readLines("./swearWords.txt",warn=FALSE, encoding = "UTF-8")
Sample <- readLines('sample.txt', encoding='UTF-8', skipNul=TRUE)
Sample <- corpus(Sample)
Sample <- tolower(Sample)
cluster
start <- Sys.time()
t <- tokens(Sample,
what="word",
remove_numbers = TRUE,
remove_punct = TRUE,
remove_url =TRUE,
remove_separators = TRUE,
remove_symbols = TRUE,
remove_twitter = TRUE,
verbose = TRUE)
# Remove stopwords
t <- tokens_replace(t,pattern =stopwords("english"),replacement=stopwords("english"))
#Set lower case for every word
t <- tokens_tolower(t)
rm(Sample)
#Apply stemmer to words
t <- tokens_wordstem(t, language = "english")
#Apply stemmer to words
t <- tokens_wordstem(t, language = "english")
t.1gram <- tokens_ngrams(t, n = 1, concatenator = " ")
t.2gram <- tokens_ngrams(t, n = 2, concatenator = " ")
t.3gram <- tokens_ngrams(t, n = 3, concatenator = " ")
t.4gram <- tokens_ngrams(t, n = 4, concatenator = " ")
t.5gram <- tokens_ngrams(t, n = 5, concatenator = " ")
t.6gram <- tokens_ngrams(t, n = 6, concatenator = " ")
t.7gram <- tokens_ngrams(t, n = 7, concatenator = " ")
unigram <- dfm(t.1gram, verbose = FALSE,remove = profanity )
bigram <- dfm(t.2gram, verbose = FALSE, remove = profanity)
trigram <- dfm(t.3gram, verbose = FALSE, remove = profanity)
quadgram <- dfm(t.4gram, verbose = FALSE, remove = profanity)
quintgram <- dfm(t.5gram, verbose = FALSE, remove = profanity)
sexgram <- dfm(t.6gram, verbose = FALSE, remove = profanity)
septgram <- dfm(t.7gram, verbose = FALSE, remove = profanity)
uniGram <- textstat_frequency(unigram)
biGram <- textstat_frequency(bigram)
triGram <- textstat_frequency(trigram)
quadGram <- textstat_frequency(quadgram)
quintGram <- textstat_frequency(quintgram)
sexGram <- textstat_frequency(sexgram)
septGram <- textstat_frequency(septgram)
end<- Sys.time()
tokentime <- end- start
write.csv(uniGram, file = 'uniGram.csv', row.names = F)
write.csv(biGram, file = 'biGram.csv', row.names = F)
write.csv(triGram, file = 'triGram.csv', row.names = F)
write.csv(quadGram, file = 'quadGram.csv', row.names = F)
write.csv(quintGram, file = 'quintGram.csv', row.names = F)
write.csv(sexGram, file = 'sexGram.csv', row.names = F)
write.csv(septGram, file = 'septGram.csv', row.names = F)
rm(c('uniGram', 'biGram', 'triGram', 'quadGram', 'quintGram', 'sexGram', 'septGram'))
generatePred <- function(inputFile, thresh = 1L) {
## This function makes the prediction look up table
## inputFile: the ngram csv file generated from quanteda
## thresh: threshold to remove low frequency words (default is 1)
nGram <- fread(inputFile, select = c('feature', 'frequency'))
nGram <- nGram[nGram$frequency > thresh]
nGram <- nGram[, query := strsplit(feature, " [^ ]+$")][]
nGram <- nGram[, predict := sub('.* (.*)$','\\1', feature)][]
fwrite(nGram, paste0(sub('.csv', '', inputFile), 'Pred.csv'))
}
# Generate nGrams (n = 2:4)
generatePred('uniGram.csv')
generatePred('biGram.csv')
generatePred('triGram.csv')
generatePred('quadGram.csv')
generatePred('quintGram.csv')
generatePred('sexGram.csv')
generatePred('septGram.csv')
uni<- read.csv('uniGramPred.csv')
names(uni)
bi<- read.csv('biGramPred.csv')
names(bi)
tri<- read.csv('triGramPred.csv')
names(tri)
quad<- read.csv('quadGramPred.csv')
names(quad)
quint<- read.csv('quintGramPred.csv')
names(tri)
six<- read.csv('sexGramPred.csv')
names(six)
sept<- read.csv('septGramPred.csv')
names(sept)
nGramPred <- rbind(uni, bi, tri, quad, quint, six, sept )
dim(nGramPred)
names(nGramPred)
write.csv(nGramPred, file = 'nGramPred.csv', row.names = F)
# Read in all predictions
nGram <- fread('nGramPred.csv', select = c('query', 'predict', 'frequency'))
nGram <- nGram[order(-frequency)]
# Filter out frequency < 5 word combinations
nGramFilt <- nGram[frequency >= 5]
fwrite(nGramFilt, file = 'predictionTableFull.csv')
# Only keep the unique queries (for predicting one word) and frequency >= 5
nGramUni <- nGram[(!duplicated(nGram$query)) & (frequency >= 5)]
fwrite(nGramUni, file = 'predictionTableUni.csv')
#cat biGramPred.csv <(sed '1d' triGramPred.csv) <(sed '1d' quadGramPred.csv) <(sed '1d' quintGramPred.csv) <(sed '1d' sexGramPred.csv) <(sed '1d' septGramPred.csv) > nGramPred.csv
suppressMessages(library(data.table))
# Read in the prediction table
nGramUni <-fread('predictionTableUni.csv')
# Write a function that takes in a string and predicts next word
nextWord <- function(rawStr) {
## [A] Remove numbers and punctuations
filtList <- gsub('[[:punct:]]|[[:digit:]]', "", tolower(rawStr))
# strsplit by all white spaces
filtList <- unlist(strsplit(filtList, "\\s+"))
## [B] Extract last 6 words for query
if (length(filtList) > 6) {
filtList <- filtList[(length(filtList)-5):length(filtList)] #make query length 6
filtStr <- paste(filtList, collapse = " ") #combine back to sentence
} else {
filtStr <- paste(filtList, collapse = " ") #combine back to sentence
}
## [C] Predicts the most likely word
predText <- nGramUni[match(filtStr, nGramUni$query), ]$predict
if (is.na(predText) == F) {
#hit with 7 gram
finalText <- predText
} else {
#no hits
filtStr <- paste(filtList[2:length(filtList)], collapse = " ") #remove 1st word
predText <- nGramUni[match(filtStr, nGramUni$query), ]$predict
if (is.na(predText) == F) {
#hit with 6 gram
finalText <- predText
} else {
#no hits
filtStr <- paste(filtList[3:length(filtList)], collapse = " ") #remove 2nd word
predText <- nGramUni[match(filtStr, nGramUni$query), ]$predict
if (is.na(predText) == F) {
#hit with 5 gram
finalText <- predText
} else {
#no hits
filtStr <- paste(filtList[4:length(filtList)], collapse = " ") #remove 3rd word
predText <- nGramUni[match(filtStr, nGramUni$query), ]$predict
if (is.na(predText) == F) {
#hit with 4 gram
finalText <- predText
} else {
#no hits
filtStr <- paste(filtList[5:length(filtList)], collapse = " ") #remove 4th word
predText <- nGramUni[match(filtStr, nGramUni$query), ]$predict
if (is.na(predText) == F) {
#hit with 3 gram
finalText <- predText
} else {
#no hits
filtStr <- paste(filtList[6:length(filtList)], collapse = " ") #remove 5th word (one word left)
predText <- nGramUni[match(filtStr, nGramUni$query), ]$predict
if (is.na(predText) == F) {
#hit with 2 gram
finalText <- predText
} else {
#no hits
finalText <- 'the' #most common word
}
}
}
}
}
}
return(finalText)
}
start <- Sys.time()
nextWord('Hello. My name is Apple and I am 2 years.')
Sys.time() - start
nextWord('i am')
nextWord('i want')
# Read in the prediction table
nGramAll <- fread('predictionTableFull.csv')
nextWords <- function(rawStr, n) {
## [A] Remove numbers and punctuations
filtList <- gsub('[[:punct:]]|[[:digit:]]', "", tolower(rawStr))
# strsplit by all white spaces
filtList <- unlist(strsplit(filtList, "\\s+"))
## [B] Extract last 6 words for query
if (length(filtList) > 6) {
filtList <- filtList[(length(filtList)-5):length(filtList)] #make query length 6
filtStr <- paste(filtList, collapse = " ") #combine back to sentence
} else {
filtStr <- paste(filtList, collapse = " ") #combine back to sentence
}
## [C] Returns all the matched words
predText <- nGramAll[filtStr == nGramAll$query, ]$predict
if (length(predText) > 0) {
#hit with 7 gram
finalText <- predText
} else {
#no hits
filtStr <- paste(filtList[2:length(filtList)], collapse = " ") #remove 1st word
predText <- nGramAll[filtStr == nGramAll$query, ]$predict
if (length(predText) > 0) {
#hit with 6 gram
finalText <- predText
} else {
#no hits
filtStr <- paste(filtList[3:length(filtList)], collapse = " ") #remove 2nd word
predText <- nGramAll[filtStr == nGramAll$query, ]$predict
if (length(predText) > 0) {
#hit with 5 gram
finalText <- predText
} else {
#no hits
filtStr <- paste(filtList[4:length(filtList)], collapse = " ") #remove 3rd word
predText <- nGramAll[filtStr == nGramAll$query, ]$predict
if (length(predText) > 0) {
#hit with 4 gram
finalText <- predText
} else {
#no hits
filtStr <- paste(filtList[5:length(filtList)], collapse = " ") #remove 4th word
predText <- nGramAll[filtStr == nGramAll$query, ]$predict
if (length(predText) > 0) {
#hit with 3 gram
finalText <- predText
} else {
#no hits
filtStr <- paste(filtList[6:length(filtList)], collapse = " ") #remove 5th word (one word left)
predText <- nGramAll[filtStr == nGramAll$query, ]$predict
if (length(predText) > 0) {
#hit with 2 gram
finalText <- predText
} else {
#no hits
finalText <- 'the' #most common word
}
}
}
}
}
}
return(finalText[1:n])
} #end of function braket
queryStr <- 'Hello. My name is Apple and I am 2 years.'
start <- Sys.time()
nextWords(queryStr, 1)
Sys.time() - start
# This model increases the prediction time for 1 word by ~0.4 seconds
# Test the function: returns 3 words with the backoff model
start <- Sys.time()
nextWords(queryStr, 3)
Sys.time() - start
#Test the function: returns 5 words with the backoff model
start <- Sys.time()
nextWords(queryStr, 5)
Sys.time() - start
nextWords('The guy in front of me just bought a pound of bacon, a bouquet, and a case of', 2)
nextWords("You're the reason why I smile everyday. Can you follow me please? It would mean the", 1)
nextWords("Hey sunshine, can you follow me and make me the", 1)
result <- nextWords("Very early observations on the Bills game: Offense still struggling but the", 1)
result
match(c('defense', 'crowd', 'players', 'referees'), result)
result <- nextWordsApprox("Very early observations on the Bills game: Offense still struggling but the", 1000)
match(c('defense', 'crowd', 'players', 'referees'), result)
nextWords('The guy in front of me just bought a pound of bacon, a bouquet, and a case of', 100)
nextWords("You're the reason why I smile everyday. Can you follow me please? It would mean the", 5)
nextWords("Hey sunshine, can you follow me and make me the", 5)
nextWords("Very early observations on the Bills game: Offense still struggling but the", 1000)
nextWords("Go on a romantic date at the", 1000)
setwd(getwd())
suppressMessages(library(data.table))
nGramAll <- fread('predictionTableFull.csv')
filtStr <- 'all the'
nGramAll[nGramAll$query %like% filtStr][20:25] #example output of substring match
nextWordsApprox <- function(rawStr, n) {
## [A] Remove numbers and punctuations
filtList <- gsub('[[:punct:]]|[[:digit:]]', "", tolower(rawStr))
# strsplit by all white spaces
filtList <- unlist(strsplit(filtList, "\\s+"))
## [B] Extract last 6 words for query
if (length(filtList) > 6) {
filtList <- filtList[(length(filtList)-5):length(filtList)] #make query length 6
filtStr <- paste(filtList, collapse = " ") #combine back to sentence
} else {
filtStr <- paste(filtList, collapse = " ") #combine back to sentence
}
## [C] Returns all the matched words
predText <- nGramAll[nGramAll$query %like% filtStr]$predict
if (length(predText) > 0) {
#hit with 7 gram
finalText <- predText
} else {
#no hits
filtStr <- paste(filtList[2:length(filtList)], collapse = " ") #remove 1st word
predText <- nGramAll[nGramAll$query %like% filtStr]$predict
if (length(predText) > 0) {
#hit with 6 gram
finalText <- predText
} else {
#no hits
filtStr <- paste(filtList[3:length(filtList)], collapse = " ") #remove 2nd word
predText <- nGramAll[nGramAll$query %like% filtStr]$predict
if (length(predText) > 0) {
#hit with 5 gram
finalText <- predText
} else {
#no hits
filtStr <- paste(filtList[4:length(filtList)], collapse = " ") #remove 3rd word
predText <- nGramAll[nGramAll$query %like% filtStr]$predict
if (length(predText) > 0) {
#hit with 4 gram
finalText <- predText
} else {
#no hits
filtStr <- paste(filtList[5:length(filtList)], collapse = " ") #remove 4th word
predText <- nGramAll[nGramAll$query %like% filtStr]$predict
if (length(predText) > 0) {
#hit with 3 gram
finalText <- predText
} else {
#no hits
filtStr <- paste(filtList[6:length(filtList)], collapse = " ") #remove 5th word (one word left)
predText <- nGramAll[nGramAll$query %like% filtStr]$predict
if (length(predText) > 0) {
#hit with 2 gram
finalText <- predText
} else {
#no hits
finalText <- 'the' #most common word
}
}
}
}
}
}
return(finalText[1:n])
} #end of function braket
queryStr <- 'Hello. My name is Apple and I am 2 years.'
start <- Sys.time()
nextWordsApprox(queryStr, 1)
Sys.time() - start
source('nextWords.R')
queryStr <- 'Hello. My name is Apple and I am 2 years.'
start <- Sys.time()
nextWords(queryStr, 1)
Sys.time() - start
result <- nextWordsApprox("Very early observations on the Bills game: Offense still struggling but the", 1000)
match(c('defense', 'crowd', 'players', 'referees'), result)
nextWordsApprox("After the ice bucket challenge Louis will push his long wet hair out of his eyes with his little", 5)
result <- nextWords("Go on a romantic date at the", 1000)
match(c('movies', 'grocery', 'beach', 'mall'), result)
result <- nextWordsApprox("his little", 100)
match(c('eyes', 'fingers', 'ears', 'toes'), result)
match(c('eye', 'finger', 'ear', 'toe'), result)
result <- nextWordsApprox("Be grateful for the good times and keep the faith during the", 5000)
match(c('hard', 'worse', 'bad', 'sad'), result)
result <- nextWordsApprox("If this isn't the cutest thing you've ever seen, then you must be", 5000)
match(c('insensitive', 'insane', 'callous', 'asleep'), result)
rm(list=ls())
setwd("~/GitHub/DataScienceSpecialization/My Learning/Course 10- Capstone/Capstone Project")
setwd("~/GitHub/DataScienceSpecialization/My Learning/Course 10- Capstone/Capstone Project/WordPred")
shiny::runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
setwd("~/GitHub/DataScienceSpecialization/My Learning/Course 10- Capstone/Capstone Project/ShinyWordPred")
runApp()
